# 2018fall llm通关总结

> Focus. Fight. Win
>
> Ash, APEX Games

这个目录中存放着2018年秋季学期, 全部公开题目的题解. 从welcome-lab 到 lab9, 前面几个手写, 2025年的全部由llm生成.

2025年9月的ICPC竞赛中, [Gemini](https://deepmind.google/discover/blog/gemini-achieves-gold-level-performance-at-the-international-collegiate-programming-contest-world-finals/)和 GPT都取得了令人满意的结果, 分别是 10/12 和 12/12 (refer: <https://simonwillison.net/2025/Sep/17/icpc/>).

Gemini使用了专有的Gemini 2.5 Deep Think 的高级版本, GPT 方面则是现有的GPT-5 (可能是GPT5-Codex?)和一个实验性推理模型一起工作.

## 模型如何训练出了这么强的推理能力

2023年的GPT4, 虽然谈吐已经很似人了, 但是codeforces上的分数几乎无法击败任何经过训练的人类选手, 只能拿到 400 分. 2024年的 O1 就可以触及 1900分, 到了现在已经立于顶端, 几乎无法被击败, 这是如何训练出来的?

算法题输入输出非常明确, 输入一份题目, 一些用例, 输出一段代码. 这段代码可以在很短时间内通过多重测试, 给出通过或者不通过的反馈. 宽松一些的话, 还可以反馈 TLE/MLE/编译失败等反馈. 这就很有利于迭代方式进行提升

整体训练过程类似于老式柴油发动机: 需要手摇启动, 启动之后就能输出强劲马力. 一个可能的步骤如下

0. 首先得有一个底座模型, 比如Gemini-2.5-pro
1. 人类选手通过对问题进行分析, 预先将一部分问题写出分析步骤, 并写出对应的代码.
2. 将步骤一中的 `题目, 用例, 思考过程, 代码` 对基础模型进行微调, 使其具备基础的算法思考能力, 能够在强化学习中入门, 不至于只能产出失败用例
3. 评估模型的算法能力, 在算法边界之上的小范围内选定新的题目
4. 进入强化学习步骤, 将题目用例送入模型, 将产出的代码测试验证, 并通过验证思路能否被与输出的代码匹配(通过相同的prompt, 送入题目, 用例, 思路, 观察能否输出正确的结果等), 对输出的代码风格进行评估等手段, 将产出的思维链-代码产物合成为一个整体 reward
5. 利用reward更新策略, 并跳回到第三步.

这个过程中, 可以在每个过程中reward较高的结果保存下来作为高质量的合成训练数据, 供后续预训练等步骤使用. 或者直接使用最后一个版本, 对过程中的所有问题进行再次解答, 产出高质量的合成数据.

## 人类如何模拟这个过程来训练自己

### 建立基础能力

+ 目标: 对一个领域内的问题有基础的解答能力
+ 方法: 通过leetcode等平台, 借助难度标注等方式, 从简单到困难, 通过快速的观察题目-得出思路-比对答案的循环, 得到对问题的抽象能力, 以及一些典型问题的处理方式

> DSAA这门课提供的题目可不太适合"入门", 你得从其他平台完成0-1这一步.

### 自我微调

+ 目标：把常见模式内化为可复用技能(例如二分、贪心、图论模板、并查集使用场合).
+ 方法：把题库按题型组织, 针对每个题型做集中练习: 先做 5-10 道相似题, 要求用不同变形/优化策略解决; 随后总结模板并形成"解题卡"与常用代码片段.
+ 输出物: 解题模板集、常见边界处理 checklist.

> 这一步有人担心遇到菱形依赖问题: 我是否会和其他人参考了相同的代码库?
>
> 如果直接对DSAA问题进行参考, 那自然是风险极高, 但是如果对经典问题进行参考, 从多个经典问题中汲取一段一段的思路, 那风险就降低很多了.

### 自动化测试

+ 目标: 把本地测试尽量自动化, 形成快速反馈回路.
+ 方法: 为每道题准备多组测试: 样例、随机生成的弱覆盖用例、以及针对边界的极端用例. 构建本地的自动测试脚本. 固化测试用例组织方式.
+ 产出: 高效的用例生成能力, 修改功能后快速测试的能力.

本质上看到题目后进入的是下面的循环

1. 读题目写代码,
2. 测试代码是否通过所有用例, 如果不通过, 跳回到1
3. 通过所有用例, 提交到OJ, 如果不通过, 构造更多测试用例, 并跳回到1
4. 如果通过, 则可以总结思路, 抽象代码为库

这一步中若能自动化步骤2, 就能极大的加速整个循环的速度.

### 人类偏好与同行评审(对应 RM 数据)

+ 目标: 引入主观质量判断(代码可读性、简洁性、思路清晰度)并获得外部反馈.
+ 方法: 加入学习小组或代码复审圈子: 定期互评彼此的题解, 采用 pairwise 比较(A 更优于 B)来总结偏好并讨论改进点. 也可以在社区(如 Code Review 频道)请求反馈.
+ 输出物: 同行评审记录、改进建议清单.

> [!NOTE]
>
> 注意不要引入压行等和软件工程相违背的风格

### 资源管理与节奏

+ 每日训练时间推荐: 1-3 小时(视学业/工作负担);

> [!NOTE]
>
> 边际收益会递减, 切换到另外的领域去获取更大的综合收益.

+ 标注与校验: 把重要的思路标注为"高价值", 定期复盘, 可以通过博客/markdown文档等方式进行沉淀.

## 这个模板仓库加速了什么?

本仓库的目标是把个人刷题与题目工程化、可复用化, 缩短"写-测-改-提交-复盘"的反馈回路, 具体体现在下面几点:

### 每个题目有独立目录

题目放在独立的子目录下, 目录内可以存放题目描述 (中/英)、个人翻译、思路笔记、版本迭代的代码以及特殊说明. 这样做的好处是: 不再需要频繁打开网页查题目, 方便离线阅读与长期积累, 同时便于把题解和注释作为学习资料分享或归档.

> 有些题目搞一大段描述, 不分段也就罢了, 题目描述还是张图片, 内部恨是吧.
>
> <https://acm.sustech.edu.cn/onlinejudge/problem.php?cid=1039>

### 在 IDE 与命令行上可通过 JUnit 快速跑通所有用例

每个题目配套的 `Main` 类和 `test` 目录下的 JUnit 测试允许你在 IDE (如 IntelliJ) 里一句 Run 即可执行全部样例. 也可以在命令行通过 `mvn test` 或 `mvn -Dtest=... test` 快速执行, 用最短的时间得到反馈 (通过/TLE/异常等), 加快本地迭代速度.

> code-agent 对命令行执行是强依赖, 单靠复制粘贴比对输出有点太繁琐了

### 统一的测试用例管理

仓库对用例做了统一命名与分组管理: 把"预期输入" (input)、"预期输出" (expected)、以及运行时产生的"测试输出" (actual)分别归档并以规范化命名存放 (例如 `01.data.in`, `01.data.out`, `01.test.out`). 这种做法便于把高质量的用例共享与复用.

### 使用 Maven/POM 简化提交到 OJ 前的改名与构建流程

提交到许多 OJ 需要把类名或包名改为特定格式(例如 `Main`), 或者只上传单个源文件. 仓库通过 `pom.xml` 和简单的脚本约定, 能把项目打包、按需生成单文件提交版本或替换 `Main` 名称, 免去每次手动重命名/复制的繁琐操作, 从而加速"本地通过->提交->得分"的循环.

### module 隔离与长期积累(把库代码与测试分离)

通过模块/目录结构把公共库 (如自实现的数据结构、模板方法)放在 `src` 中, 把单元测试与题目驱动放在 `test` 中. 这样既保持了代码的可维护性, 也方便把好用的手写数据结构逐步积累成可复用的工具箱, 未来可以在新题中直接复用、单测或优化.

### In conclusion

总之, 该模板把"写题"和"工程化测试/复盘"结合起来, 目标是用工程化的手段把题目练习变成可重复、可追溯、可分享的学习循环, 显著提高个人刷题与总结的效率.

## LLM工具

本次使用了三种工具

### GitHub Copilot

GitHub 的学生包中 <https://github.com/education/students> 提供了免费的 Copilot Pro, 每月三百次高级调用, 实测下来GPT5-Mini这个免费调用的模型能够秒杀easy/middle问题, 1x费率的高级模型可以通过几次迭代解决hard问题.

GitHub Copilot还可以登录 `opencode auth login`, 用Copilot的额度体验cli

### Gemini-cli

使用方式参考 <https://blog.certseeds.com/2025/connect_gemini_cli.html>

整体上来说不太满意, 虽然免费额度很高(一天一千次对话), 但是gemini-cli进行多轮对话并不能显著的提高coding能力, 经常能给出基础N^2实现, 但是无法成功的优化出时间复杂度达到预期的产物. 好消息是工具调用能力挺强, 可以产出大量的基础测试用例, 通过基础实现给出正确的输出, 供优化产物调试用.

### GPT-5 chatroom

一些非常困难的问题, gemini/opus均无法得出正确的结论, 反而是GPT5能够在思考几分钟之后得出结论, 确实非常强, 可以通过 openrouter 的 chatroom 获取, 便宜大碗.